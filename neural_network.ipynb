{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dce41e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My notes from watching https://www.youtube.com/watch?v=PaCmpygFfXo\n",
    "# The spelled-out intro to language modeling: building makemore by Andrej Karpathy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d079bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc478131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast the bigram character level language modeling into neural network framework\n",
    "# it receives a single character as input, then there is a neural network with some weights(parameters) w\n",
    "# and then it will output the probability distribution over the next character in the sequence\n",
    "# which character is likely to follow the input character?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4769dc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". e\n",
      "e m\n",
      "m m\n",
      "m a\n",
      "a .\n"
     ]
    }
   ],
   "source": [
    "# first create the training set of bigrams (x, y)\n",
    "# we are given x, we are trying to predict y\n",
    "\n",
    "xs, ys = [], []\n",
    "\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "\n",
    "\n",
    "# finally add the start / end token\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "for w in words[:1]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        print(ch1, ch2)\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "        \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08ae4550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cf95f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4eddb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for \"emma\", there are 5 separate input examples\n",
    "# when the input is integer 0, the desired label is integer 5.\n",
    "# when the input is 5, we want the weights to be arranged so that 13 gets a very high probability\n",
    "# when 13 is input, we want 13 to have a high probability\n",
    "# when 13 is input, we also want 1 to have a high probability\n",
    "# when 1 is input, we want 0 to have a high probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46d81218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is torch.tensor\n",
    "# there is torch.Tensor (tensor class that you can construct)\n",
    "# what's the difference? \n",
    "# torch.tensor infers the dtype while torch.Tensor returns a float\n",
    "# advised that you use torch.tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "005389c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "# we want to cast to float for some reason to feed into the neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d5218d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d24a321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fde487d9520>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd716b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c399c22a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1818],\n",
       "        [ 1.0574],\n",
       "        [-1.6936],\n",
       "        [-1.6936],\n",
       "        [-0.2548]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the initial weights \n",
    "W = torch.randn((27, 1))\n",
    "\n",
    "# @ is the matrix multiplication operator in PyTorch\n",
    "xenc @ W\n",
    "\n",
    "# (5, 27) @ (27, 1) becomes (5, 1)\n",
    "\n",
    "# We see the 5 activations of this neuron on these 5 inputs. \n",
    "# We evaluated all 5 in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "329176a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4377,  0.5300, -0.7810,  1.3994, -0.7344, -0.3805, -0.0635,  2.6279,\n",
       "          0.5014, -0.4229, -0.1932,  0.1376, -2.1939, -1.0828, -0.1622,  0.6783,\n",
       "          0.1765, -0.1066, -0.1666, -2.1751,  1.1102,  1.9043, -0.4629, -1.4501,\n",
       "         -0.0537,  0.5480, -0.8174],\n",
       "        [-0.7962, -0.2061, -0.6220,  0.2610,  0.3534, -1.4845,  0.3060,  0.2525,\n",
       "         -1.2855, -1.2617,  1.2533,  0.3231,  1.6391,  0.3305, -1.5431, -2.1455,\n",
       "         -0.4458,  0.8281, -0.2135, -0.7406,  0.2737, -0.9459,  0.0191, -0.9473,\n",
       "         -1.9009, -0.1800, -1.1357],\n",
       "        [ 0.2377,  1.6512, -0.2919, -0.0889,  1.1622, -0.7896, -1.0883,  1.2449,\n",
       "         -0.9099, -1.4007, -0.3196,  0.3046, -1.0319,  0.6429, -3.0464,  1.2666,\n",
       "          2.8703,  1.3266,  0.8574,  0.4311,  0.7702, -1.2435, -1.9238, -0.7378,\n",
       "         -1.3359,  0.1369, -0.3770],\n",
       "        [ 0.2377,  1.6512, -0.2919, -0.0889,  1.1622, -0.7896, -1.0883,  1.2449,\n",
       "         -0.9099, -1.4007, -0.3196,  0.3046, -1.0319,  0.6429, -3.0464,  1.2666,\n",
       "          2.8703,  1.3266,  0.8574,  0.4311,  0.7702, -1.2435, -1.9238, -0.7378,\n",
       "         -1.3359,  0.1369, -0.3770],\n",
       "        [ 1.7134,  0.3373, -0.3514,  1.5852,  1.1613,  0.6191, -1.2866,  0.0032,\n",
       "          0.4546,  1.0103,  0.7815, -0.9039, -0.1352,  1.3751, -0.3038,  1.4242,\n",
       "          0.9230,  2.4565,  0.8092, -0.3641, -0.8412,  0.9127,  0.4270,  0.1759,\n",
       "         -1.1733,  0.3794, -1.7572]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the initial weights - but now use 27\n",
    "W = torch.randn((27, 27))\n",
    "\n",
    "# @ is the matrix multiplication operator in PyTorch\n",
    "xenc @ W\n",
    "\n",
    "# (5, 27) @ (27, 27) -> (5, 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66bae936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xenc @ W).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0e231fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6429)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for every 27 neurons, what is the firing rate \n",
    "\n",
    "# example\n",
    "(xenc @ W)[3, 13] # is giving us the firing rate of the 13th neuron looking at the 3rd input \n",
    "# it was achieved using the dot product "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f237637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99e29a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0828,  1.3751,  1.5616, -0.8406, -1.2671,  0.3305,  0.9033, -0.0066,\n",
       "         1.9868, -0.5488,  0.4259, -1.3165, -1.6594,  0.6429,  0.4055, -0.0476,\n",
       "        -0.6964,  0.5815,  0.3601,  1.2575,  0.2349, -0.0515, -1.4433, -1.4497,\n",
       "        -0.4865,  1.9008, -1.8381])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[:,  13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca28814b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6429)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xenc[3] * W[:, 13]).sum() # the dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "218291c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.2111,  1.6989,  0.4579,  4.0526,  0.4798,  0.6836,  0.9385, 13.8445,\n",
       "          1.6510,  0.6552,  0.8244,  1.1476,  0.1115,  0.3387,  0.8502,  1.9705,\n",
       "          1.1931,  0.8989,  0.8465,  0.1136,  3.0350,  6.7150,  0.6294,  0.2345,\n",
       "          0.9478,  1.7297,  0.4416],\n",
       "        [ 0.4511,  0.8137,  0.5369,  1.2983,  1.4239,  0.2266,  1.3580,  1.2872,\n",
       "          0.2765,  0.2832,  3.5020,  1.3814,  5.1506,  1.3917,  0.2137,  0.1170,\n",
       "          0.6403,  2.2889,  0.8078,  0.4768,  1.3148,  0.3883,  1.0193,  0.3878,\n",
       "          0.1494,  0.8353,  0.3212],\n",
       "        [ 1.2683,  5.2134,  0.7469,  0.9150,  3.1968,  0.4540,  0.3368,  3.4724,\n",
       "          0.4026,  0.2464,  0.7264,  1.3560,  0.3563,  1.9020,  0.0475,  3.5487,\n",
       "         17.6420,  3.7682,  2.3571,  1.5389,  2.1601,  0.2884,  0.1461,  0.4782,\n",
       "          0.2629,  1.1467,  0.6859],\n",
       "        [ 1.2683,  5.2134,  0.7469,  0.9150,  3.1968,  0.4540,  0.3368,  3.4724,\n",
       "          0.4026,  0.2464,  0.7264,  1.3560,  0.3563,  1.9020,  0.0475,  3.5487,\n",
       "         17.6420,  3.7682,  2.3571,  1.5389,  2.1601,  0.2884,  0.1461,  0.4782,\n",
       "          0.2629,  1.1467,  0.6859],\n",
       "        [ 5.5479,  1.4012,  0.7037,  4.8802,  3.1940,  1.8572,  0.2762,  1.0032,\n",
       "          1.5756,  2.7463,  2.1848,  0.4050,  0.8735,  3.9554,  0.7380,  4.1543,\n",
       "          2.5167, 11.6637,  2.2462,  0.6948,  0.4312,  2.4911,  1.5326,  1.1924,\n",
       "          0.3093,  1.4614,  0.1725]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# neural net outputs the log counts\n",
    "# to get the counts, you need to exponentiate the log counts\n",
    "\n",
    "# exponent (exp(x) \n",
    "# element-wise exponent of each, they turn into positive numbers\n",
    "# all the negative numbers turned into numbers below 1\n",
    "# all the positive numbers turned into even more positive numbers (above 1)\n",
    "\n",
    "(xenc @ W).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6820107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0831, 0.0335, 0.0090, 0.0799, 0.0095, 0.0135, 0.0185, 0.2731, 0.0326,\n",
       "         0.0129, 0.0163, 0.0226, 0.0022, 0.0067, 0.0168, 0.0389, 0.0235, 0.0177,\n",
       "         0.0167, 0.0022, 0.0599, 0.1324, 0.0124, 0.0046, 0.0187, 0.0341, 0.0087],\n",
       "        [0.0159, 0.0287, 0.0189, 0.0458, 0.0502, 0.0080, 0.0479, 0.0454, 0.0098,\n",
       "         0.0100, 0.1236, 0.0487, 0.1817, 0.0491, 0.0075, 0.0041, 0.0226, 0.0808,\n",
       "         0.0285, 0.0168, 0.0464, 0.0137, 0.0360, 0.0137, 0.0053, 0.0295, 0.0113],\n",
       "        [0.0232, 0.0954, 0.0137, 0.0167, 0.0585, 0.0083, 0.0062, 0.0635, 0.0074,\n",
       "         0.0045, 0.0133, 0.0248, 0.0065, 0.0348, 0.0009, 0.0649, 0.3227, 0.0689,\n",
       "         0.0431, 0.0282, 0.0395, 0.0053, 0.0027, 0.0087, 0.0048, 0.0210, 0.0125],\n",
       "        [0.0232, 0.0954, 0.0137, 0.0167, 0.0585, 0.0083, 0.0062, 0.0635, 0.0074,\n",
       "         0.0045, 0.0133, 0.0248, 0.0065, 0.0348, 0.0009, 0.0649, 0.3227, 0.0689,\n",
       "         0.0431, 0.0282, 0.0395, 0.0053, 0.0027, 0.0087, 0.0048, 0.0210, 0.0125],\n",
       "        [0.0921, 0.0233, 0.0117, 0.0811, 0.0530, 0.0308, 0.0046, 0.0167, 0.0262,\n",
       "         0.0456, 0.0363, 0.0067, 0.0145, 0.0657, 0.0123, 0.0690, 0.0418, 0.1937,\n",
       "         0.0373, 0.0115, 0.0072, 0.0414, 0.0255, 0.0198, 0.0051, 0.0243, 0.0029]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = xenc @ W # log-counts \n",
    "counts = logits.exp() # this is equivalent to N from the bigram notebook\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities are just the counts normalized\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7e193d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in probs, every row will sum to 1, since they are normalized\n",
    "probs[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4c190b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape\n",
    "# for every one of our 5 examples, we now have a row that came out of a neural net, and because of the transformations \n",
    "# we made sure we can interpret the results to be probabilities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f47993cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0831, 0.0335, 0.0090, 0.0799, 0.0095, 0.0135, 0.0185, 0.2731, 0.0326,\n",
       "        0.0129, 0.0163, 0.0226, 0.0022, 0.0067, 0.0168, 0.0389, 0.0235, 0.0177,\n",
       "        0.0167, 0.0022, 0.0599, 0.1324, 0.0124, 0.0046, 0.0187, 0.0341, 0.0087])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0]\n",
    "# how likely every one of these characters is to come next in the sequence after '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0a805ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we find a good W so that we get good probabilities?\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba4d9adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f45f5603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "273bde5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50c5b7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly initialize 27 neuron's weights. each neuron receives 27 inputs.\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "557ed2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict the log counts\n",
    "counts = logits.exp() # get the counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "# btw the last two lines here are together called a 'softmax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2da94f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd92389e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "bigram example 1: .e (indexes 0,5)\n",
      "input to the neural net: 0\n",
      "output probabilities from the neural net: tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
      "        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
      "        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459])\n",
      "label (actual next character): 5\n",
      "probability assigned by the neural net to the correct character 0.01228625513613224\n",
      "log likelihood: -4.399273872375488\n",
      "negative log likelihood: 4.399273872375488\n",
      "-----\n",
      "bigram example 2: em (indexes 5,13)\n",
      "input to the neural net: 5\n",
      "output probabilities from the neural net: tensor([0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n",
      "        0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n",
      "        0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472])\n",
      "label (actual next character): 13\n",
      "probability assigned by the neural net to the correct character 0.018050700426101685\n",
      "log likelihood: -4.014570713043213\n",
      "negative log likelihood: 4.014570713043213\n",
      "-----\n",
      "bigram example 3: mm (indexes 13,13)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
      "label (actual next character): 13\n",
      "probability assigned by the neural net to the correct character 0.026691533625125885\n",
      "log likelihood: -3.623408794403076\n",
      "negative log likelihood: 3.623408794403076\n",
      "-----\n",
      "bigram example 4: ma (indexes 13,1)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
      "label (actual next character): 1\n",
      "probability assigned by the neural net to the correct character 0.07367686182260513\n",
      "log likelihood: -2.6080665588378906\n",
      "negative log likelihood: 2.6080665588378906\n",
      "-----\n",
      "bigram example 5: a. (indexes 1,0)\n",
      "input to the neural net: 1\n",
      "output probabilities from the neural net: tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
      "        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
      "        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091])\n",
      "label (actual next character): 0\n",
      "probability assigned by the neural net to the correct character 0.014977526850998402\n",
      "log likelihood: -4.201204299926758\n",
      "negative log likelihood: 4.201204299926758\n",
      "=====\n",
      "average negative log likelihood, i.e. loss = 3.7693049907684326\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nlls = torch.zeros(5)\n",
    "for i in range(5):\n",
    "    # i-th bigram\n",
    "    x = xs[i].item() # input character index\n",
    "    y = ys[i].item() # label character index\n",
    "    print('-----')\n",
    "    print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x},{y})')\n",
    "    print('input to the neural net:', x)\n",
    "    print('output probabilities from the neural net:', probs[i])\n",
    "    print('label (actual next character):', y)\n",
    "    p = probs[i, y]\n",
    "    print('probability assigned by the neural net to the correct character', p.item())\n",
    "    logp = torch.log(p)\n",
    "    print('log likelihood:', logp.item())\n",
    "    nll = -logp\n",
    "    print('negative log likelihood:', nll.item())\n",
    "    nlls[i] = nll\n",
    "    \n",
    "print('=====')\n",
    "print('average negative log likelihood, i.e. loss =', nlls.mean().item())\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c7382df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopped at 1:31:48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c4e9f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
