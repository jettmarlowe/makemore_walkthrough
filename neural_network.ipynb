{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dce41e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My notes from watching https://www.youtube.com/watch?v=PaCmpygFfXo\n",
    "# The spelled-out intro to language modeling: building makemore by Andrej Karpathy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d079bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc478131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast the bigram character level language modeling into neural network framework\n",
    "# it receives a single character as input, then there is a neural network with some weights(parameters) w\n",
    "# and then it will output the probability distribution over the next character in the sequence\n",
    "# which character is likely to follow the input character?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4769dc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". e\n",
      "e m\n",
      "m m\n",
      "m a\n",
      "a .\n"
     ]
    }
   ],
   "source": [
    "# first create the training set of bigrams (x, y)\n",
    "# we are given x, we are trying to predict y\n",
    "\n",
    "xs, ys = [], []\n",
    "\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "\n",
    "\n",
    "# finally add the start / end token\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "for w in words[:1]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        print(ch1, ch2)\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "        \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08ae4550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cf95f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4eddb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for \"emma\", there are 5 separate input examples\n",
    "# when the input is integer 0, the desired label is integer 5.\n",
    "# when the input is 5, we want the weights to be arranged so that 13 gets a very high probability\n",
    "# when 13 is input, we want 13 to have a high probability\n",
    "# when 13 is input, we also want 1 to have a high probability\n",
    "# when 1 is input, we want 0 to have a high probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46d81218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is torch.tensor\n",
    "# there is torch.Tensor (tensor class that you can construct)\n",
    "# what's the difference? \n",
    "# torch.tensor infers the dtype while torch.Tensor returns a float\n",
    "# advised that you use torch.tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "005389c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "# we want to cast to float for some reason to feed into the neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d5218d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d24a321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7facd0b06280>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd716b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c399c22a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8759],\n",
       "        [ 0.1881],\n",
       "        [-0.7261],\n",
       "        [-0.7261],\n",
       "        [-0.8975]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the initial weights \n",
    "W = torch.randn((27, 1))\n",
    "\n",
    "# @ is the matrix multiplication operator in PyTorch\n",
    "xenc @ W\n",
    "\n",
    "# (5, 27) @ (27, 1) becomes (5, 1)\n",
    "\n",
    "# We see the 5 activations of this neuron on these 5 inputs. \n",
    "# We evaluated all 5 in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "329176a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7549, -0.2560,  0.4110, -0.2763, -0.4027, -0.0456,  0.3778, -0.0035,\n",
       "         -2.2907,  1.5003, -1.1770,  0.9562, -1.2603,  0.9471,  2.2595, -1.4124,\n",
       "         -0.3648, -0.3643, -0.7721,  0.8234, -0.7387, -0.2529,  1.0256,  0.0266,\n",
       "         -1.4382,  1.0847,  0.6374],\n",
       "        [-0.8454, -1.3705,  0.2044,  0.0421, -0.1453,  1.7296,  0.2699,  1.2937,\n",
       "         -0.7770,  0.6460,  1.0555,  0.5840, -0.6558,  0.1481,  1.1280,  0.4013,\n",
       "         -1.4661, -0.2125, -1.2857, -0.5621,  0.2863, -0.3678,  1.0126,  1.1377,\n",
       "          0.0635, -0.5563, -1.2956],\n",
       "        [ 0.3443,  1.5892, -2.0530, -0.2815, -0.4280, -1.0523,  2.6247, -1.6462,\n",
       "         -0.0510, -0.2136, -0.4569, -0.3232, -0.8954, -0.8619, -1.6132, -0.3040,\n",
       "          1.6852, -0.2986,  0.4578,  2.0407,  1.5253, -1.1166,  0.6858,  1.6992,\n",
       "          0.6451,  0.4714,  1.9416],\n",
       "        [ 0.3443,  1.5892, -2.0530, -0.2815, -0.4280, -1.0523,  2.6247, -1.6462,\n",
       "         -0.0510, -0.2136, -0.4569, -0.3232, -0.8954, -0.8619, -1.6132, -0.3040,\n",
       "          1.6852, -0.2986,  0.4578,  2.0407,  1.5253, -1.1166,  0.6858,  1.6992,\n",
       "          0.6451,  0.4714,  1.9416],\n",
       "        [ 0.3079,  0.1654, -0.6299,  1.3778, -1.1768,  1.4521,  0.5693, -0.2156,\n",
       "         -0.0599,  0.9792,  1.0790, -1.3143, -1.0647,  0.9152,  1.8394,  2.5064,\n",
       "         -0.0301, -0.4054,  0.5013, -0.4069,  1.4819,  0.0707, -0.6177, -0.0568,\n",
       "          1.6442,  0.0134,  0.2213]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the initial weights - but now use 27\n",
    "W = torch.randn((27, 27))\n",
    "\n",
    "# @ is the matrix multiplication operator in PyTorch\n",
    "xenc @ W\n",
    "\n",
    "# (5, 27) @ (27, 27) -> (5, 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66bae936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xenc @ W).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0e231fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.8619)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for every 27 neurons, what is the firing rate \n",
    "\n",
    "# example\n",
    "(xenc @ W)[3, 13] # is giving us the firing rate of the 13th neuron looking at the 3rd input \n",
    "# it was achieved using the dot product "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f237637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99e29a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.9471,  0.9152,  0.0667, -0.5274, -0.5689,  0.1481, -0.1493,  0.4015,\n",
       "         0.3164,  0.6870, -2.7756,  0.4780,  0.0194, -0.8619,  0.2691,  0.1839,\n",
       "        -0.3898, -0.5072, -1.3861, -0.5847, -2.0564,  1.5997,  0.4040, -1.2443,\n",
       "         0.6524, -0.6651, -1.5207])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[:,  13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca28814b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.8619)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xenc[3] * W[:, 13]).sum() # the dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "218291c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4700,  0.7742,  1.5083,  0.7586,  0.6685,  0.9555,  1.4591,  0.9965,\n",
       "          0.1012,  4.4831,  0.3082,  2.6019,  0.2836,  2.5782,  9.5783,  0.2436,\n",
       "          0.6943,  0.6947,  0.4620,  2.2782,  0.4778,  0.7766,  2.7887,  1.0269,\n",
       "          0.2374,  2.9585,  1.8916],\n",
       "        [ 0.4294,  0.2540,  1.2268,  1.0430,  0.8647,  5.6384,  1.3099,  3.6463,\n",
       "          0.4598,  1.9080,  2.8733,  1.7932,  0.5190,  1.1596,  3.0894,  1.4938,\n",
       "          0.2308,  0.8086,  0.2765,  0.5700,  1.3316,  0.6923,  2.7528,  3.1195,\n",
       "          1.0656,  0.5733,  0.2737],\n",
       "        [ 1.4111,  4.9000,  0.1283,  0.7547,  0.6518,  0.3491, 13.8001,  0.1928,\n",
       "          0.9503,  0.8077,  0.6332,  0.7238,  0.4084,  0.4224,  0.1993,  0.7378,\n",
       "          5.3933,  0.7419,  1.5806,  7.6958,  4.5963,  0.3274,  1.9854,  5.4693,\n",
       "          1.9062,  1.6022,  6.9699],\n",
       "        [ 1.4111,  4.9000,  0.1283,  0.7547,  0.6518,  0.3491, 13.8001,  0.1928,\n",
       "          0.9503,  0.8077,  0.6332,  0.7238,  0.4084,  0.4224,  0.1993,  0.7378,\n",
       "          5.3933,  0.7419,  1.5806,  7.6958,  4.5963,  0.3274,  1.9854,  5.4693,\n",
       "          1.9062,  1.6022,  6.9699],\n",
       "        [ 1.3605,  1.1799,  0.5326,  3.9661,  0.3083,  4.2720,  1.7669,  0.8060,\n",
       "          0.9418,  2.6622,  2.9417,  0.2686,  0.3448,  2.4972,  6.2926, 12.2607,\n",
       "          0.9704,  0.6667,  1.6508,  0.6657,  4.4012,  1.0733,  0.5392,  0.9448,\n",
       "          5.1769,  1.0135,  1.2477]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# neural net outputs the log counts\n",
    "# to get the counts, you need to exponentiate the log counts\n",
    "\n",
    "# exponent (exp(x) \n",
    "# element-wise exponent of each, they turn into positive numbers\n",
    "# all the negative numbers turned into numbers below 1\n",
    "# all the positive numbers turned into even more positive numbers (above 1)\n",
    "\n",
    "(xenc @ W).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6820107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0112, 0.0184, 0.0359, 0.0180, 0.0159, 0.0227, 0.0347, 0.0237, 0.0024,\n",
       "         0.1066, 0.0073, 0.0619, 0.0067, 0.0613, 0.2278, 0.0058, 0.0165, 0.0165,\n",
       "         0.0110, 0.0542, 0.0114, 0.0185, 0.0663, 0.0244, 0.0056, 0.0703, 0.0450],\n",
       "        [0.0109, 0.0064, 0.0311, 0.0265, 0.0219, 0.1431, 0.0332, 0.0925, 0.0117,\n",
       "         0.0484, 0.0729, 0.0455, 0.0132, 0.0294, 0.0784, 0.0379, 0.0059, 0.0205,\n",
       "         0.0070, 0.0145, 0.0338, 0.0176, 0.0699, 0.0792, 0.0270, 0.0146, 0.0069],\n",
       "        [0.0216, 0.0750, 0.0020, 0.0115, 0.0100, 0.0053, 0.2112, 0.0030, 0.0145,\n",
       "         0.0124, 0.0097, 0.0111, 0.0063, 0.0065, 0.0030, 0.0113, 0.0825, 0.0114,\n",
       "         0.0242, 0.1178, 0.0703, 0.0050, 0.0304, 0.0837, 0.0292, 0.0245, 0.1067],\n",
       "        [0.0216, 0.0750, 0.0020, 0.0115, 0.0100, 0.0053, 0.2112, 0.0030, 0.0145,\n",
       "         0.0124, 0.0097, 0.0111, 0.0063, 0.0065, 0.0030, 0.0113, 0.0825, 0.0114,\n",
       "         0.0242, 0.1178, 0.0703, 0.0050, 0.0304, 0.0837, 0.0292, 0.0245, 0.1067],\n",
       "        [0.0224, 0.0194, 0.0088, 0.0653, 0.0051, 0.0703, 0.0291, 0.0133, 0.0155,\n",
       "         0.0438, 0.0484, 0.0044, 0.0057, 0.0411, 0.1036, 0.2018, 0.0160, 0.0110,\n",
       "         0.0272, 0.0110, 0.0724, 0.0177, 0.0089, 0.0156, 0.0852, 0.0167, 0.0205]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = xenc @ W # log-counts \n",
    "counts = logits.exp() # this is equivalent to N from the bigram notebook\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities are just the counts normalized\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7e193d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in probs, every row will sum to 1, since they are normalized\n",
    "probs[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4c190b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape\n",
    "# for every one of our 5 examples, we now have a row that came out of a neural net, and because of the transformations \n",
    "# we made sure we can interpret the results to be probabilities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f47993cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0112, 0.0184, 0.0359, 0.0180, 0.0159, 0.0227, 0.0347, 0.0237, 0.0024,\n",
       "        0.1066, 0.0073, 0.0619, 0.0067, 0.0613, 0.2278, 0.0058, 0.0165, 0.0165,\n",
       "        0.0110, 0.0542, 0.0114, 0.0185, 0.0663, 0.0244, 0.0056, 0.0703, 0.0450])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0]\n",
    "# how likely every one of these characters is to come next in the sequence after '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0a805ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we find a good W so that we get good probabilities?\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba4d9adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f45f5603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "273bde5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50c5b7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly initialize 27 neuron's weights. each neuron receives 27 inputs.\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "557ed2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict the log counts\n",
    "counts = logits.exp() # get the counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "# btw the last two lines here are together called a 'softmax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2da94f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd92389e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "bigram example 1: .e (indexes 0,5)\n",
      "input to the neural net: 0\n",
      "output probabilities from the neural net: tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
      "        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
      "        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459])\n",
      "label (actual next character): 5\n",
      "probability assigned by the neural net to the correct character 0.01228625513613224\n",
      "log likelihood: -4.399273872375488\n",
      "negative log likelihood: 4.399273872375488\n",
      "-----\n",
      "bigram example 2: em (indexes 5,13)\n",
      "input to the neural net: 5\n",
      "output probabilities from the neural net: tensor([0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n",
      "        0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n",
      "        0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472])\n",
      "label (actual next character): 13\n",
      "probability assigned by the neural net to the correct character 0.018050700426101685\n",
      "log likelihood: -4.014570713043213\n",
      "negative log likelihood: 4.014570713043213\n",
      "-----\n",
      "bigram example 3: mm (indexes 13,13)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
      "label (actual next character): 13\n",
      "probability assigned by the neural net to the correct character 0.026691533625125885\n",
      "log likelihood: -3.623408794403076\n",
      "negative log likelihood: 3.623408794403076\n",
      "-----\n",
      "bigram example 4: ma (indexes 13,1)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
      "label (actual next character): 1\n",
      "probability assigned by the neural net to the correct character 0.07367686182260513\n",
      "log likelihood: -2.6080665588378906\n",
      "negative log likelihood: 2.6080665588378906\n",
      "-----\n",
      "bigram example 5: a. (indexes 1,0)\n",
      "input to the neural net: 1\n",
      "output probabilities from the neural net: tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
      "        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
      "        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091])\n",
      "label (actual next character): 0\n",
      "probability assigned by the neural net to the correct character 0.014977526850998402\n",
      "log likelihood: -4.201204299926758\n",
      "negative log likelihood: 4.201204299926758\n",
      "=====\n",
      "average negative log likelihood, i.e. loss = 3.7693049907684326\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nlls = torch.zeros(5)\n",
    "for i in range(5):\n",
    "    # i-th bigram\n",
    "    x = xs[i].item() # input character index\n",
    "    y = ys[i].item() # label character index\n",
    "    print('-----')\n",
    "    print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x},{y})')\n",
    "    print('input to the neural net:', x)\n",
    "    print('output probabilities from the neural net:', probs[i])\n",
    "    print('label (actual next character):', y)\n",
    "    p = probs[i, y]\n",
    "    print('probability assigned by the neural net to the correct character', p.item())\n",
    "    logp = torch.log(p)\n",
    "    print('log likelihood:', logp.item())\n",
    "    nll = -logp\n",
    "    print('negative log likelihood:', nll.item())\n",
    "    nlls[i] = nll\n",
    "    \n",
    "print('=====')\n",
    "print('average negative log likelihood, i.e. loss =', nlls.mean().item())\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7382df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopped at 1:31:48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c4e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can change the W to get a better or worse model\n",
    "# To Optimize a neural network\n",
    "# start with some random guess \n",
    "# now we have a loss function (which is made up of only differentiable operations)\n",
    "# so we can minimize the loss by tuning W \n",
    "# by computing the gradients of the loss wrt to the W\n",
    "\n",
    "# loss is a single number that summarized the performance of the neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8ff25f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we are doing classification so we are using the negative log likelihood\n",
    "\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "70b80e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0123),\n",
       " tensor(0.0181),\n",
       " tensor(0.0267),\n",
       " tensor(0.0737),\n",
       " tensor(0.0150))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0, 5], probs[1, 13], probs[2, 13], probs[3, 1], probs[4, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e99044fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.7693)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to do that in PyTorch we can pass in the integers in a vector\n",
    "\n",
    "loss = -probs[torch.arange(5), ys].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d2bc2357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly initialize 27 neuron's weights. each neuron receives 27 inputs.\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f140e342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict the log counts\n",
    "counts = logits.exp() # get the counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "loss = -probs[torch.arange(5), ys].log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4f360cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass\n",
    "W.grad = None  # set the gradient to zero (setting to None is more efficient than setting to zero)\n",
    "loss.backward() # this fills in the gradients to all the intermediates all the way back to W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4796290d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 27])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "083e45ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 27])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6004fdce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0122,  0.0020,  0.0025,  0.0008,  0.0034, -0.1974,  0.0005,  0.0046,\n",
       "          0.0027,  0.0063,  0.0016,  0.0056,  0.0018,  0.0016,  0.0100,  0.0473,\n",
       "          0.0121,  0.0005,  0.0050,  0.0011,  0.0068,  0.0022,  0.0006,  0.0040,\n",
       "          0.0024,  0.0307,  0.0291],\n",
       "        [-0.1969,  0.0017,  0.0079,  0.0020,  0.0121,  0.0062,  0.0216,  0.0026,\n",
       "          0.0025,  0.0010,  0.0204,  0.0017,  0.0197,  0.0022,  0.0046,  0.0041,\n",
       "          0.0082,  0.0016,  0.0180,  0.0106,  0.0093,  0.0062,  0.0010,  0.0066,\n",
       "          0.0131,  0.0101,  0.0018],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0058,  0.0159,  0.0050,  0.0104,  0.0396,  0.0058,  0.0019,  0.0067,\n",
       "          0.0019,  0.0060,  0.0140,  0.0046,  0.0023, -0.1962,  0.0022,  0.0063,\n",
       "          0.0058,  0.0009,  0.0183,  0.0043,  0.0097,  0.0060,  0.0100,  0.0005,\n",
       "          0.0024,  0.0004,  0.0094],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0125, -0.1695,  0.0193,  0.0133,  0.0268,  0.0080,  0.0105,  0.0100,\n",
       "          0.0486,  0.0066,  0.0030,  0.0314,  0.0052, -0.1889,  0.0059,  0.0045,\n",
       "          0.0233,  0.0049,  0.0259,  0.0023,  0.0083,  0.0031,  0.0053,  0.0081,\n",
       "          0.0478,  0.0187,  0.0051],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.grad\n",
    "# every element of W.grad is telling us the influence of that weight on the loss function\n",
    "\n",
    "# for example, W[0, 0] is 0.0121\n",
    "# since it is positive it is telling us that it has a positive influence on the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5faf93a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7291419506073\n"
     ]
    }
   ],
   "source": [
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "89df6d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use the gradient information to update the weights of the neural network\n",
    "# do the update to the tensor\n",
    "\n",
    "W.data += -0.1 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0b05958b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.7291, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d290a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when we achieve a low loss it will mean that the network is assigning high probabilities to the correct next characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1d4ad2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  228146\n"
     ]
    }
   ],
   "source": [
    "### Putting it all together\n",
    "\n",
    "# create the data set\n",
    "xs, ys = [], []\n",
    "\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "        \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)   \n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the network\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5d310c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4726529121398926\n",
      "2.4724340438842773\n",
      "2.4722204208374023\n",
      "2.472010850906372\n",
      "2.4718058109283447\n",
      "2.4716053009033203\n",
      "2.4714088439941406\n",
      "2.4712166786193848\n",
      "2.4710280895233154\n",
      "2.470843553543091\n",
      "2.4706625938415527\n",
      "2.4704854488372803\n",
      "2.470311403274536\n",
      "2.4701414108276367\n",
      "2.4699745178222656\n",
      "2.4698104858398438\n",
      "2.4696500301361084\n",
      "2.469492197036743\n",
      "2.4693377017974854\n",
      "2.4691858291625977\n",
      "2.4690372943878174\n",
      "2.468891143798828\n",
      "2.468747615814209\n",
      "2.468606472015381\n",
      "2.468468189239502\n",
      "2.4683327674865723\n",
      "2.4681992530822754\n",
      "2.4680681228637695\n",
      "2.4679393768310547\n",
      "2.4678127765655518\n",
      "2.46768856048584\n",
      "2.4675662517547607\n",
      "2.4674463272094727\n",
      "2.4673285484313965\n",
      "2.467211961746216\n",
      "2.467097759246826\n",
      "2.4669857025146484\n",
      "2.4668753147125244\n",
      "2.466766595840454\n",
      "2.4666597843170166\n",
      "2.466554641723633\n",
      "2.466451406478882\n",
      "2.4663491249084473\n",
      "2.4662487506866455\n",
      "2.4661505222320557\n",
      "2.4660532474517822\n",
      "2.4659576416015625\n",
      "2.4658634662628174\n",
      "2.4657704830169678\n",
      "2.465679407119751\n",
      "2.4655892848968506\n",
      "2.465500593185425\n",
      "2.4654135704040527\n",
      "2.465327262878418\n",
      "2.465242624282837\n",
      "2.4651589393615723\n",
      "2.4650766849517822\n",
      "2.4649953842163086\n",
      "2.4649155139923096\n",
      "2.464836359024048\n",
      "2.46475887298584\n",
      "2.464682102203369\n",
      "2.464606523513794\n",
      "2.4645321369171143\n",
      "2.4644582271575928\n",
      "2.464385509490967\n",
      "2.4643139839172363\n",
      "2.464243173599243\n",
      "2.4641733169555664\n",
      "2.464104652404785\n",
      "2.464036703109741\n",
      "2.4639699459075928\n",
      "2.4639039039611816\n",
      "2.4638383388519287\n",
      "2.463773727416992\n",
      "2.463710308074951\n",
      "2.4636473655700684\n",
      "2.463585376739502\n",
      "2.463524103164673\n",
      "2.463463544845581\n",
      "2.4634039402008057\n",
      "2.4633445739746094\n",
      "2.4632863998413086\n",
      "2.463228702545166\n",
      "2.4631717205047607\n",
      "2.4631154537200928\n",
      "2.463060140609741\n",
      "2.4630050659179688\n",
      "2.4629504680633545\n",
      "2.4628970623016357\n",
      "2.462843894958496\n",
      "2.462791681289673\n",
      "2.462739944458008\n",
      "2.462688684463501\n",
      "2.4626379013061523\n",
      "2.462587594985962\n",
      "2.462538242340088\n",
      "2.462489128112793\n",
      "2.4624407291412354\n",
      "2.462392807006836\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(100):\n",
    "    \n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "    logits = xenc @ W # predict the log counts\n",
    "    counts = logits.exp() # get the counts, equivalent to N\n",
    "    probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    loss = -probs[torch.arange(num), ys].log().mean()\n",
    "    print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    W.grad = None # set to zero the gradient\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "   # W.data += -0.1 * W.grad  # -0.1 is the learning rate\n",
    "    W.data += -50 * W.grad  # try a larger learning rate\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be45c392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what number do we expect to get for the loss?\n",
    "# similar to what we had originally when we did it by counting (2.47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bc767e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1080, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# regularization loss\n",
    "# adds like a spring force that pushes W to be zero in the loss function\n",
    "\n",
    "(W**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "25c250c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mor.\n",
      "axwaninaymoryles.\n",
      "kondlaisah.\n",
      "anchshizarie.\n",
      "odaren.\n"
     ]
    }
   ],
   "source": [
    "# finally sample from the neural net model\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    \n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        \n",
    "        # BEFORE\n",
    "        # p = P[ix]\n",
    "        # NOW\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "        logits = xenc @ W # predict the log counts\n",
    "        counts = logits.exp() # get the counts, equivalent to N\n",
    "        p = counts/ counts.sum(1, keepdims=True) # probabilities for next character\n",
    "        \n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "        \n",
    "    print(''.join(out))\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
